
# Its tiny Python HTTP server used as the “simulator model” for the tenant example.
# The script listens on port 8080 and implements POST /v1/chat/completions,
# returning a canned OpenAI-style JSON response (with sample token counts).
apiVersion: v1
kind: ConfigMap
metadata:
  name: sim-app
data:
  sim.py: |
    import json
    from http.server import BaseHTTPRequestHandler, HTTPServer
    class H(BaseHTTPRequestHandler):
        def do_POST(self):
            if self.path == "/v1/chat/completions":
                length = int(self.headers.get('Content-Length', '0'))
                body = self.rfile.read(length) if length else b'{}'
                try:
                    req = json.loads(body.decode("utf-8"))
                except Exception:
                    req = {}
                try:
                    msg = req.get("messages", [{}])[-1].get("content","")
                except Exception:
                    msg = ""
                resp = {
                    "id": "chatcmpl-sim",
                    "object": "chat.completion",
                    "created": 0,
                    "model": req.get("model","simulator-model"),
                    "choices": [{
                        "index": 0,
                        "message": {"role": "assistant",
                                    "content": f"This is a simulated response to: {msg}"},
                        "finish_reason": "stop"}],
                    "usage": {"prompt_tokens": 10, "completion_tokens": 20, "total_tokens": 30}
                }
                self.send_response(200)
                self.send_header("Content-Type","application/json")
                self.end_headers()
                self.wfile.write(json.dumps(resp).encode("utf-8"))
            else:
                self.send_response(404); self.end_headers()
    if __name__ == "__main__":
        HTTPServer(("0.0.0.0", 8080), H).serve_forever()
